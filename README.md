# Real vs Fake Face Classification using CNNs 🧠🖼

This project is a deep learning-based image classification application developed using Python and Convolutional Neural Networks (CNNs). The model is capable of distinguishing between **real** and **fake** human face images, with a particular focus on detecting high-quality fake faces generated by modern GAN architectures such as **StyleGAN2**.

---

## 🔍 Introduction

With the rapid advancement of artificial intelligence and deep learning technologies, detecting fake face images has become a significant challenge. These images can be maliciously used in identity fraud, misinformation, and deepfake generation.

The goal of this project is to evaluate the performance of various lightweight CNN architectures in classifying real and fake face images using two distinct datasets:

- A relatively **easy** dataset with less complex fake samples.
- A **hard** dataset containing highly realistic fake faces that are difficult to distinguish, even for the human eye.

Models used in this study:

- MobileNet v3-small
- MobileNet v3-large
- SqueezeNet
- ShuffleNet
- ResNet
- AlexNet
- VGG19
- Custom CNN model

---

## 📁 Datasets

### 1. Real-vs-Fake (RVF10K)
- **Source**: [Kaggle - RVF10K](https://www.kaggle.com/datasets/sachchitkunichetty/rvf10k)
- **Content**: Contains images labeled as either real or fake (generated via GANs).
- **Structure**: Organized into `real` and `fake` subfolders.

### 2. Hard Fake vs Real Faces
- **Source**: [Kaggle - Hard Fake vs Real Faces](https://www.kaggle.com/datasets/hamzaboulahia/hardfakevsrealfaces)
- **Content**: Contains extremely realistic fake faces created with **StyleGAN2**.
- **Challenge**: Fake samples are more difficult to classify correctly.

Both datasets were split into:
- 70% Training
- 20% Validation
- 10% Test

---

## ⚙️ Methodology

1. **Preprocessing**:
   - Image resizing and normalization
   - Label encoding
   - Train-validation-test split

2. **Data Augmentation**:
   - Random rotations
   - Horizontal flips
   - Zoom and brightness adjustments
   - Aimed at preventing overfitting and improving model generalization

3. **Model Training**:
   - Multiple CNN architectures were trained separately on both datasets.
   - Training conducted using PyTorch/TensorFlow 

4. **Evaluation Metrics**:
   - Accuracy
   - Confusion Matrix
   - Validation Loss
   - Precision / Recall / F1 

---

## 📊 Results & Comparison

Below is a comparison table of the accuracy scores obtained for each model across both datasets:

---

### 🧪 Dataset 1: Hard Fake vs Real Faces (StyleGAN2)

| Model              | Best Epoch | Train Accuracy | Val Accuracy | Train Loss | Val Loss | Optimizer | LR     | Notes                            |
|-------------------|------------|----------------|--------------|------------|----------|-----------|--------|----------------------------------|
| MobileNet v3-Small| 20/20      | 1.0000         | 1.0000       | 0.0028     | 0.0015   | SGD       | 0.001  | momentum=0.9, batch size=4, augmentation used  |
| MobileNet v3-Large| 17/20      | 0.9978         | 1.0000       | 0.0096     | 0.0039   | SGD       | 0.001  | momentum=0.9, batch size=4                     |
| SqueezeNet        | 19/20      | 1.0000         | 0.9961       | 0.0000     | 0.0354   | SGD       | 0.001  | momentum=0.9, batch size=4                     |
| ShuffleNet        | 17/20      | 0.9645         | 1.0000       | 0.1362     | 0.0264   | SGD       | 0.001  | momentum=0.9, batch size=4                     |

---

### 🧪 Dataset 2: Real-vs-Fake Faces (RVF10K) – SqueezeNet  Trials

|  Trials    | Epoch | Optimizer | LR     | Train Acc | Val Acc | Train Loss | Val Loss | Notes |
|------------|-------|-----------|--------|-----------|---------|------------|----------|-------|
| #1         | 9/20  | SGD       | 0.001  | 0.7427    | 0.6055  | 0.5095     | 0.7628   | augmentation used |
| #2         | 20/20 | SGD       | 0.0001 | 0.7277    | 0.5898  | 0.5439     | 0.7399   | Complex augmentation |
| #3         | 8/20  | Adam      | 0.0001 | 0.8104    | 0.6309  | 0.4092     | 0.7129   | augmentation used |
| #4         | 20/20 | SGD       | 0.0001 | 0.7460    | 0.6172  | 0.5254     | 0.6944   | augmentation used, Dropout, BN, dynamic LR |
| #5         | 16/20 | SGD       | 0.0001 | 0.7143    | 0.6113  | 0.5621     | 0.6762   | augmentation used, Weight decay added |

> ⚙️ Batch size was set to **32**.

---
### 🧪 Dataset 2: Real-vs-Fake Faces (RVF10K) – ShuffleNet Trials

| Trial | Epoch  | Optimizer | LR       | Train Acc | Val Acc | Train Loss | Val Loss | Notes                                      |
|-------|--------|-----------|----------|-----------|---------|------------|----------|--------------------------------------------|
| #1    | 6/20   | SGD       | 0.001    | 0.5556    | 0.5547  | 0.6864     | 0.6869   | early stopping, batch size=32              |
| #2    | 9/20   | Adam      | 0.0001   | 0.9674    | 0.6797  | 0.1353     | 0.8471   | batch size=32                              |
| #3    | 20/20  | SGD       | 0.0001   | 0.7477    | 0.6523  | 0.5026     | 0.6553   | advanced augmentation, batch size=32       |
| #4    | 24/25  | SGD       | 0.0001   | 0.8312    | 0.6113  | 0.4062     | 0.7566   | dropout, batch size=16                     |
| #5    | 20/20  | SGD       | 0.0001   | 0.7778    | 0.6387  | 0.4638     | 0.6902   | dropout, batch size=32                     |
| #6    | 21/30  | SGD       | 0.00001  | 0.6700    | 0.6094  | 0.6113     | 0.6504   | batch size=8                               |
| #7    | 16/30  | SGD       | 0.0001   | 0.6182    | 0.5938  | 0.6578     | 0.6653   | batch size=128                             |
| #8    | 12/30  | SGD       | 0.000001 | 0.4294    | 0.4590  | 0.9224     | 0.8684   | dropout, early stopping, batch size=128    |

> 📌 Data augmentation techniques were applied in all experiments to improve generalization and avoid overfitting.
---

## 💻 Technologies Used

- Python 3.x
- PyTorch 
- TensorFlow 
- OpenCV
- NumPy & Pandas
- Matplotlib & Seaborn
- Scikit-learn

---

## 📌 Key Takeaways

- Lightweight CNN models can effectively distinguish between real and fake faces.
- StyleGAN2-generated images pose significant challenges for classifiers.
- Data augmentation significantly improves generalization and reduces overfitting.
- Hard datasets reveal model limitations, highlighting the need for ongoing research.

---

## 🤝 Collaborators

This project was developed in collaboration with:

- 👩‍💻 [@beyzakutuk](https://github.com/beyzakutuk) 
- 👩‍💻 [@RanaBetulKaya](https://github.com/RanaBetulKaya)
- 👩‍💻 [@edakorkusuz](https://github.com/edakorkusuz)

Special thanks to my teammates for their contributions in data preparation, experimentation, and evaluation phases.
